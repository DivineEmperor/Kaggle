{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Assuming the first column is the label\n",
    "train_labels = train_data.iloc[:, 0].values\n",
    "train_features = train_data.iloc[:, 1:].values.reshape(-1, 28, 28).astype('float32')\n",
    "\n",
    "# Split train data for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set for Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self, features, labels=None, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx].reshape(28, 28, 1)  # Reshape to 28x28\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = CustomDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = CustomDataset(test_data.values.reshape(-1, 28, 28).astype('float32'), transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=1000, shuffle=False,drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing model, optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "one_batch = next(iter(val_loader))\n",
    "\n",
    "# Check the output\n",
    "print(type(one_batch))  # Should be a tuple\n",
    "print(one_batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    for batch_idx,batch  in enumerate(train_loader):\n",
    "\n",
    "        data = batch[0]\n",
    "        labels = batch[1]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "\n",
    "        loss = criterion(out,labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)} ({100. * correct / len(val_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# Training and testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/33600 (0%)]\tLoss: 45.958023\n",
      "Train Epoch: 1 [6400/33600 (19%)]\tLoss: 0.171683\n",
      "Train Epoch: 1 [12800/33600 (38%)]\tLoss: 0.093988\n",
      "Train Epoch: 1 [19200/33600 (57%)]\tLoss: 0.207935\n",
      "Train Epoch: 1 [25600/33600 (76%)]\tLoss: 0.078447\n",
      "Train Epoch: 1 [32000/33600 (95%)]\tLoss: 0.124650\n",
      "Train Epoch: 2 [0/33600 (0%)]\tLoss: 0.174908\n",
      "Train Epoch: 2 [6400/33600 (19%)]\tLoss: 0.166218\n",
      "Train Epoch: 2 [12800/33600 (38%)]\tLoss: 0.043634\n",
      "Train Epoch: 2 [19200/33600 (57%)]\tLoss: 0.115782\n",
      "Train Epoch: 2 [25600/33600 (76%)]\tLoss: 0.054686\n",
      "Train Epoch: 2 [32000/33600 (95%)]\tLoss: 0.233528\n",
      "Train Epoch: 3 [0/33600 (0%)]\tLoss: 0.046297\n",
      "Train Epoch: 3 [6400/33600 (19%)]\tLoss: 0.035829\n",
      "Train Epoch: 3 [12800/33600 (38%)]\tLoss: 0.028259\n",
      "Train Epoch: 3 [19200/33600 (57%)]\tLoss: 0.140524\n",
      "Train Epoch: 3 [25600/33600 (76%)]\tLoss: 0.068254\n",
      "Train Epoch: 3 [32000/33600 (95%)]\tLoss: 0.097789\n",
      "Train Epoch: 4 [0/33600 (0%)]\tLoss: 0.058639\n",
      "Train Epoch: 4 [6400/33600 (19%)]\tLoss: 0.004078\n",
      "Train Epoch: 4 [12800/33600 (38%)]\tLoss: 0.153874\n",
      "Train Epoch: 4 [19200/33600 (57%)]\tLoss: 0.003718\n",
      "Train Epoch: 4 [25600/33600 (76%)]\tLoss: 0.020513\n",
      "Train Epoch: 4 [32000/33600 (95%)]\tLoss: 0.094846\n",
      "Train Epoch: 5 [0/33600 (0%)]\tLoss: 0.015279\n",
      "Train Epoch: 5 [6400/33600 (19%)]\tLoss: 0.005746\n",
      "Train Epoch: 5 [12800/33600 (38%)]\tLoss: 0.011747\n",
      "Train Epoch: 5 [19200/33600 (57%)]\tLoss: 0.008551\n",
      "Train Epoch: 5 [25600/33600 (76%)]\tLoss: 0.045524\n",
      "Train Epoch: 5 [32000/33600 (95%)]\tLoss: 0.012903\n",
      "Train Epoch: 6 [0/33600 (0%)]\tLoss: 0.111268\n",
      "Train Epoch: 6 [6400/33600 (19%)]\tLoss: 0.022995\n",
      "Train Epoch: 6 [12800/33600 (38%)]\tLoss: 0.020714\n",
      "Train Epoch: 6 [19200/33600 (57%)]\tLoss: 0.183059\n",
      "Train Epoch: 6 [25600/33600 (76%)]\tLoss: 0.002367\n",
      "Train Epoch: 6 [32000/33600 (95%)]\tLoss: 0.002729\n",
      "Train Epoch: 7 [0/33600 (0%)]\tLoss: 0.073117\n",
      "Train Epoch: 7 [6400/33600 (19%)]\tLoss: 0.012054\n",
      "Train Epoch: 7 [12800/33600 (38%)]\tLoss: 0.006587\n",
      "Train Epoch: 7 [19200/33600 (57%)]\tLoss: 0.036476\n",
      "Train Epoch: 7 [25600/33600 (76%)]\tLoss: 0.132514\n",
      "Train Epoch: 7 [32000/33600 (95%)]\tLoss: 0.001374\n",
      "Train Epoch: 8 [0/33600 (0%)]\tLoss: 0.016408\n",
      "Train Epoch: 8 [6400/33600 (19%)]\tLoss: 0.013042\n",
      "Train Epoch: 8 [12800/33600 (38%)]\tLoss: 0.001013\n",
      "Train Epoch: 8 [19200/33600 (57%)]\tLoss: 0.001189\n",
      "Train Epoch: 8 [25600/33600 (76%)]\tLoss: 0.165305\n",
      "Train Epoch: 8 [32000/33600 (95%)]\tLoss: 0.039341\n",
      "Train Epoch: 9 [0/33600 (0%)]\tLoss: 0.069689\n",
      "Train Epoch: 9 [6400/33600 (19%)]\tLoss: 0.002479\n",
      "Train Epoch: 9 [12800/33600 (38%)]\tLoss: 0.073713\n",
      "Train Epoch: 9 [19200/33600 (57%)]\tLoss: 0.011978\n",
      "Train Epoch: 9 [25600/33600 (76%)]\tLoss: 0.047306\n",
      "Train Epoch: 9 [32000/33600 (95%)]\tLoss: 0.001389\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 7851/8400 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            predictions.extend(pred.tolist())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'ImageId': range(1, len(test_predictions) + 1),  # ImageId starts at 1, not 0\n",
    "    'Label': [pred[0] for pred in test_predictions]  # Assuming test_predictions is a list of lists\n",
    "})\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "predictions_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
